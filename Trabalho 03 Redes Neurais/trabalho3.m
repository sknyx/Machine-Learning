%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Instituto Federal de BrasÌlia - Campus Taguatinga
% Curso: Bacharelado em CiÍncia da ComputaÁ„o
% Disciplina: Aprendizagem de M·quina - 2/2018
% Professor: Lucas Moreira

% Estudante: Carolina AtaÌde de Assis
% MatrÌcula: 161057600051
% PerÌodo: 6∞ semestre
% Data: 11/11/2018

% Trabalho 03 - Redes Neurais

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%  Complete os seguintes scripts
%
%     sigmoidGradient.m OK
%     randInitializeWeights.m OK
%     nnCostFunction.m OK

clear ; close all; clc

%% Par√¢metros iniciais da RNA
input_layer_size  = 400;  % Imagem de 20x20 pixels
hidden_layer_size = 25;   % Escolha o n√∫mero de neur√¥nios da camada oculta
num_labels = 10;          % 10 classes, de 1 a 10   
                          % (note que o r√≥tulo "0" foi mapeado como 10)

% Carrega os dados de treinamento
load('trab3data.mat');
m = size(X, 1);

% Seleciona 100 imagens aleat√≥rias para visualiza√ß√£o
sel = randperm(size(X, 1));
sel = sel(1:100);

displayData(X(sel, :));

% Um conjunto de pesos foi inicialmente estabelecida.
% Ao carregar esse arquivo, √© poss√≠vel debugar o script
% da fun√ß√£o-custo

load('trab3pesos.mat');

% transforma as matrizes de pesos em um √∫nico vetor
nn_params = [Theta1(:) ; Theta2(:)];

% Para ajudar a debugar a propaga√ß√£o direta, use a vari√°vel
% nn_params acima. Use-a para calcular o valor da fun√ß√£o-custo
% dos dados de entrada. Essas matrizes s√£o provenientes de um
% treinamento pr√©vio. O c√≥digo abaixo s√≥ funcionar√° se o c√≥digo
% da fun√ß√£o nnCostFunction() estiver implementado.

lambda = 0;

J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...
                   num_labels, X, y, lambda);

fprintf(['Sua fun√ß√£o-custo tem o valor: %f '...
         '\n(Esse valor deve ser por volta de 0.287629)\n'], J);

% O mesmo procedimento √© feito para debugar a fun√ß√£o-custo
% com regulariza√ß√£o. Implemente o termo de regulariza√ß√£o
% no script da fun√ß√£o antes de executar as linhas de c√≥digo
% abaixo.

lambda = 1;

J = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, ...
                   num_labels, X, y, lambda);

fprintf(['Sua fun√ß√£o-custo tem o valor: %f '...
         '\n(Esse valor deve ser por volta de 0.383770)\n'], J);

% Use a fun√ß√£o randInitializeWeights() para criar as duas
% matrizes de pesos com valores iniciais aleat√≥rios. Leia
% o help da fun√ß√£o para saber como us√°-la. Ap√≥s isso, √©
% poss√≠vel iniciar o treinamento da rede chamando a fun√ß√£o
% fminunc(). N√£o esque√ßa de transformar as matrizes de pesos
% em um √∫nico vetor. Tente diferentes valores de itera√ß√µes e
% e lambda.

options = optimset('GradObj','on','MaxIter', 100);

% insira seu c√≥digo a partir daqui

function p = predict(Theta1, Theta2, X)

m = size(X, 1);
num_labels = size(Theta2, 1);

p = zeros(size(X, 1), 1);

h1 = sigmoid([ones(m, 1) X] * Theta1');
h2 = sigmoid([ones(m, 1) h1] * Theta2');
[Y, p] = max(h2, [], 2);

end

%inicializaÁ„o
ini_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);
ini_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);

costFunction = @(p) nnCostFunction(p, input_layer_size, hidden_layer_size, num_labels, X, y, lambda);

ini_nn_params = [ini_Theta1(:) ; ini_Theta2(:)];

%treinamento da rede chamando a fun√ß√£o fminunc()
fprintf('\nTreinamento da Neural Network... \n')
[nn_params, cost] = fminunc(costFunction, ini_nn_params, options);

Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...
                 hidden_layer_size, (input_layer_size + 1));

Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...
                 num_labels, (hidden_layer_size + 1));

%visualizaÁ„o                 
fprintf('\nVisualizacao...\n')

displayData(Theta1(:, 2:end));

pred = predict(Theta1, Theta2, X);

fprintf('\nTraining Set Accuracy: %f\n', mean(double(pred == y)) * 100);
